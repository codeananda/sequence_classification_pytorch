{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Unaugmented Data Module\n",
    "\n",
    "We've made a basline model. Now it's time to build our first neural network. But first, let's get the data processing pipeline set up. \n",
    "\n",
    "I will use put everything together into a [PyTorch Lightning Data Module](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html) becuase they encapsulate all the data processing into one class and make it effortless to try out differnet models and perform the data processing in one step.\n",
    "\n",
    "At a later stage, I will perform data augmentation. Thus, this first Data Module (DM) is the `UnaugmentedAnalyteDataModule`. \n",
    "\n",
    "The goal here is to build our first LSTM neural network with all the data to see how it compares to the baseline classifiers. \n",
    "\n",
    "Once we have a first model, we can go back and tune it. One thing we will test is using a subset of the data. Garbage in, garbage out, so let's make sure only the best data is going into the model to ensure maximum learning. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class UnaugmentedAnalyteDataModule(pl.LightningDataModule):\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\t\t\t data_dir='data/',\n",
    "\t\t\t\t batch_size=50,\n",
    "\t\t\t\t seq_length=1002,\n",
    "\t\t\t\t rescaled_min_val=-1,\n",
    "\t\t\t\t rescaled_max_val=1,\n",
    "\t\t\t\t random_seed=42,\n",
    "\t\t\t\t shuffle=True,\n",
    "\t\t\t\t validation_split=0.2,\n",
    "\t\t\t\t ):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.data_dir = Path(data_dir)\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.seq_length = seq_length\n",
    "\t\t# Scaling params\n",
    "\t\tself.rescaled_min_val = rescaled_min_val\n",
    "\t\tself.rescaled_max_val = rescaled_max_val\n",
    "\t\tself.seed = random_seed\n",
    "\t\tself.shuffle = shuffle\n",
    "\t\tself.val_split = validation_split\n",
    "\n",
    "\n",
    "\tdef prepare_data(self):\n",
    "\t\t\"\"\"\n",
    "\t\tUse this method to do things that might write to disk\n",
    "\t\tor that need to be done only from a single process.\n",
    "\t\te.g. downloading and tokenizing data.\n",
    "\n",
    "\t\tNot needed for this project.\n",
    "\t\t\"\"\"\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "\tdef _load_dfX_dfy(self):\n",
    "        \"\"\"Read in all analytes and return df_X and df_y.\n",
    "        \"\"\"\n",
    "        analytes = pd.read_csv(DATA_DIR / 'all_data.csv', index_col=0)\n",
    "\n",
    "\t\tdf_X = df.iloc[:, :-1]\n",
    "\t\tdf_y = df.iloc[:, -1]\n",
    "\n",
    "\t\treturn df_X, df_y\n",
    "\n",
    "    def _scale_X_y(self, df_X, df_y, scaled_min=-1, scaled_max=1):\n",
    "        \"\"\"Scale each row in df_X to be in the range [scaled_min, scaled_max].\n",
    "\t\t\n",
    "        Note: the min value of df_X is mapped to scaled_min and the max is mapped\n",
    "\t\t\t  to scaled_max. Each row is not mapped to [scaled_min, scaled_max] \n",
    "              independently.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_X : pd.DataFrame\n",
    "            DataFrame where each row is a sample and each column a voltage\n",
    "            index.\n",
    "        df_y : pd.DataFrame\n",
    "            DataFrame with one column containing int labels for each sample\n",
    "        scaled_min : int, optional\n",
    "            The value df.min().min() is mapped to. The new global min for the \n",
    "            dataset as a whole, by default -1\n",
    "        scaled_max : int, optional\n",
    "            The value df.max().max() is mapped to. The new global max for the\n",
    "            dataset as a whole, by default 1\n",
    "\n",
    "        Returns \n",
    "        ----------\n",
    "        df_X_scaled: pd.DataFrame\n",
    "            DataFrame with each value scaled to sit in the range [scaled_min, scaled_max]\n",
    "        df_y: pd.DataFrame\n",
    "            df_y unmodified. \n",
    "        \"\"\"\n",
    "        df_X_scaled = self._scale_df_X_to_range(df_X, scaled_min, scaled_max)\n",
    "\t\t# No need to scale y\n",
    "\t\treturn df_X_scaled, df_y\n",
    "\n",
    "\tdef _scale_df_to_range(\n",
    "            self, \n",
    "            df, \n",
    "            df_global_min=-40, \n",
    "            df_global_max=40,\n",
    "            scaled_min=-1, \n",
    "            scaled_max=1\n",
    "            ):\n",
    "        \"\"\"Scale all rows in df to [scaled_min, scaled_max]. df_global_min is\n",
    "        mapped to scaled_min and df_global_max is mapped to scaled_max. \n",
    "        All values inbetween are mapped to their appropriate values.\n",
    "\n",
    "        Note: This scales the entire dataset row-wise. Each row is not mapped to \n",
    "              [scaled_min, scaled_max] independently.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            DataFrame where each row is a sample and each column a feature.\n",
    "        df_global_min: int, optional\n",
    "            The global minimum of the DataFrame. \n",
    "        df_global_max: int, optional\n",
    "            The global maximum of the DataFrame.\n",
    "        scaled_min : int, optional\n",
    "            The value df_global_min is mapped to. The new global min for the \n",
    "            dataset as a whole, by default -1\n",
    "        scaled_max : int, optional\n",
    "            The value df_global_max is mapped to. The new global max for the\n",
    "            dataset as a whole, by default 1\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_scaled: pd.DataFrame\n",
    "            Scaled version of df.\n",
    "        \"\"\"\n",
    "\t\tscaled_rows = []\n",
    "\t\tfor row in df.itertuples(index=False):\n",
    "\t\t\tscaled_row = self._scale_to_range(row,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  scaled_min,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  scaled_max,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  seq_min=df_global_min,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  seq_max=df_global_max)\n",
    "\t\t\tscaled_rows.append(scaled_row)\n",
    "\t\tdf_scaled = pd.DataFrame(scaled_rows, columns=df_X.columns)\n",
    "\t\treturn df_scaled\n",
    "\n",
    "\t# Taken from this SO answer: https://tinyurl.com/j5rppewr\n",
    "\tdef _scale_to_range(self,\n",
    "\t\t\t\t\t\tseq,\n",
    "\t\t\t\t\t\tscaled_min,\n",
    "\t\t\t\t\t\tscaled_max,\n",
    "\t\t\t\t\t\tseq_min=None,\n",
    "\t\t\t\t\t\tseq_max=None):\n",
    "\t\t\"\"\"\n",
    "\t\tGiven a sequence of numbers - seq - scale all of its values to the\n",
    "\t\trange [scaled_min, scaled_max].\n",
    "\n",
    "\t\tDefault behaviour will map min(seq) to scaled_min and max(seq) to\n",
    "\t\tscaled_max. To override this, set scaled_min and scaled_max yourself.\n",
    "\t\t\"\"\"\n",
    "\t\tassert scaled_min < scaled_max\n",
    "\t\t# Default is to use the max of the seq as the min/max\n",
    "\t\t#Â Can override this and input custom min and max values\n",
    "\t\t# if, for example, want to scale to ranges not necesarily included\n",
    "\t\t# in the data (as in our case with the train and val data)\n",
    "\t\tif seq_max is None:\n",
    "\t\t\tseq_max = np.max(seq)\n",
    "\t\tif seq_min is None:\n",
    "\t\t\tseq_min = np.min(seq)\n",
    "\t\tassert seq_min < seq_max\n",
    "\t\tscaled_seq = np.array([self._scale_one_value(value, scaled_min, scaled_max,\n",
    "\t\t\t\t\t\t\t\t\t\t  \t\t\t seq_min, seq_max) \\\n",
    "\t\t\t\t\t\t\t   for value in seq])\n",
    "\n",
    "\t\treturn scaled_seq\n",
    "\n",
    "\n",
    "\tdef _scale_one_value(self,\n",
    "\t\t\t\t\t\t value,\n",
    "\t\t\t\t\t\t scaled_min,\n",
    "\t\t\t\t\t\t scaled_max,\n",
    "\t\t\t\t\t\t original_min,\n",
    "\t\t\t\t\t\t original_max):\n",
    "\t\t# Scale value into [scaled_min, scaled_max] given the max and min values of the seq\n",
    "\t\t# it belongs to.\n",
    "\t\t# Taken from this SO answer: https://tinyurl.com/j5rppewr\n",
    "\t\tnumerator = (scaled_max - scaled_min) * (value - original_min)\n",
    "\t\tdenominator = original_max - original_min\n",
    "\t\treturn (numerator / denominator) + scaled_min\n",
    "\n",
    "\n",
    "\tdef _reshape(self, df_X, df_y):\n",
    "\t\t\"\"\"\n",
    "\t\tRe-shapes df_X and df_y into a format PyTorch LSTMs will accept.\n",
    "\t\tNamely: (batch, timesteps, features) - just like with Keras.\n",
    "\n",
    "\t\tNote: you must set batch_first=True in your LSTM layers for this X\n",
    "\t\t\t  shape to work.\n",
    "\t\t\"\"\"\n",
    "\t\tX = df_X.values\n",
    "\t\t# Correct if batch_first=True in nn.LSTM layers\n",
    "\t\tX = X.reshape(-1, self.seq_length, 1)\n",
    "\t\t# PyTorch accepts integer y-values by default\n",
    "\t\ty_values = df_y.values\n",
    "\t\t# label_enc = LabelEncoder()\n",
    "\t\t# y = label_enc.fit_transform(y_values)\n",
    "\t\tohe = OneHotEncoder(sparse=False)\n",
    "\t\ty = ohe.fit_transform(y_values.reshape(-1, 1))\n",
    "\t\treturn X, y\n",
    "\n",
    "\n",
    "\tdef _split_X_y(self, X, y):\n",
    "\t\tX_train, X_val, y_train, y_val = train_test_split(\n",
    "\t\t\t\t\t\t\t\t\t\t\tX, y,\n",
    "\t\t\t\t\t\t\t\t\t\t\ttest_size=self.val_split,\n",
    "\t\t\t\t\t\t\t\t\t\t\trandom_state=self.seed,\n",
    "\t\t\t\t\t\t\t\t\t\t\tshuffle=self.shuffle,\n",
    "\t\t\t\t\t\t\t\t\t\t\tstratify=y)\n",
    "\t\treturn X_train, X_val, y_train, y_val\n",
    "\n",
    "\n",
    "\tdef setup(self, stage):\n",
    "\t\t\"\"\"\n",
    "\t\tThings you want to perform on every GPU.\n",
    "\n",
    "\t\tIt is ok to assign things here.\n",
    "\t\t\"\"\"\n",
    "\t\t# Load, scale, reshape\n",
    "\t\tdf_X_unscaled, df_y = self._load_X_y_from_columns()\n",
    "\t\tdf_X_scaled, df_y = self._scale_X_y(df_X_unscaled, df_y,\n",
    "\t\t\t\t\t\t\t\t\t\t\tscaled_min=self.rescaled_min_val,\n",
    "\t\t\t\t\t\t\t\t\t\t\tscaled_max=self.rescaled_max_val)\n",
    "\t\tX, y = self._reshape(df_X_scaled, df_y)\n",
    "\t\t# Split\n",
    "\t\tself.X_train, self.X_val, \\\n",
    "\t\t\tself.y_train, self.y_val = self._split_X_y(X, y)\n",
    "\n",
    "\n",
    "\tdef train_dataloader(self):\n",
    "\t\t# Create augmented Dataset\n",
    "\t\tdataset = ElectroAugmenterDataset(\n",
    "\t\t\tself.X_train,\n",
    "\t\t\tself.y_train,\n",
    "\t\t\tself.horizontal_shift,\n",
    "\t\t\tself.vertical_shift,\n",
    "\t\t\tself.noise_shift,\n",
    "\t\t\tself.noise_shift_scale,\n",
    "\t\t\tself.multiplier,\n",
    "\t\t\tself.seed, # use same seed throughout\n",
    "\t\t\tself.aug_pct\n",
    "\t\t)\n",
    "\t\t# Create DataLoader\n",
    "\t\ttrain_loader = DataLoader(\n",
    "\t\t\tdataset,\n",
    "\t\t\tself.batch_size,\n",
    "\t\t\tself.shuffle,\n",
    "\t\t\tnum_workers=os.cpu_count()\n",
    "\t\t)\n",
    "\t\treturn train_loader\n",
    "\n",
    "\n",
    "\tdef val_dataloader(self):\n",
    "\t\tself.X_val = torch.FloatTensor(self.X_val)\n",
    "\t\tself.y_val = torch.LongTensor(self.y_val)\n",
    "\n",
    "\t\tdataset = TensorDataset(self.X_val, self.y_val)\n",
    "\n",
    "\t\tval_loader = DataLoader(\n",
    "\t\t\tdataset,\n",
    "\t\t\tself.batch_size,\n",
    "\t\t\tshuffle=False,\n",
    "\t\t\tnum_workers=os.cpu_count()\n",
    "\t\t)\n",
    "\t\treturn val_loader\n",
    "\n",
    "\n",
    "\tdef test_dataloader(self):\n",
    "\t\traise NotImplementedError('Currently does not support test_dataloader')\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "b60a01ecf532b2f759cce798974887fb1836b7786c6743edae9ea9f1cf50a2f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}