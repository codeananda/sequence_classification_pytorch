{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline Classifier\n",
    "\n",
    "In this notebook, I will create two baseline classifiers. One random and one intelligent that will make predictions based off the peak of the sample in the ranges I found in 02_DataExploration.ipynb. \n",
    "\n",
    "The peaks for each analyte are at:\n",
    "- Copper - [660, 720]\n",
    "- Cadmium - [530, 580]\n",
    "- Lead - [580, 620]\n",
    "- Seawater - N/A (it's a flat baseline)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Classifier\n",
    "\n",
    "Let's see how the model will perform if it guesses the classes 0-3 at random."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from functools import partial\n",
    "\n",
    "from scripts.data_processing import get_class_label_to_int_mapping\n",
    "from scripts.baseline import RandomClassifier, MaxPeakClassifier, \\\n",
    "                             print_scores, print_kfold_scores\n",
    "\n",
    "CLASS_LABEL_TO_INT_MAPPING = get_class_label_to_int_mapping()\n",
    "\n",
    "sns.set()\n",
    "\n",
    "DATA_DIR = Path('data/')\n",
    "\n",
    "# Read in dataframes - good to keep them separate to make plotting easier\n",
    "cadmium = pd.read_csv(DATA_DIR / 'cadmium.csv', index_col=0)\n",
    "copper = pd.read_csv(DATA_DIR / 'copper.csv', index_col=0)\n",
    "lead = pd.read_csv(DATA_DIR / 'lead.csv', index_col=0)\n",
    "seawater = pd.read_csv(DATA_DIR / 'seawater.csv', index_col=0)\n",
    "\n",
    "analytes = pd.read_csv(DATA_DIR / 'all_data.csv', index_col=0)\n",
    "\n",
    "X = analytes.drop('label', axis=1)\n",
    "y = analytes.loc[:, 'label']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "random_clf = RandomClassifier()\n",
    "print('RANDOM CLASSIFIER SCORES')\n",
    "print_scores(random_clf, X, y)\n",
    "print()\n",
    "\n",
    "max_peak_clf = MaxPeakClassifier()\n",
    "print('MAX PEAK CLASSIFIER SCORES')\n",
    "print_scores(max_peak_clf, X, y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RANDOM CLASSIFIER SCORES\n",
      "accuracy - 0.2571\n",
      "f1_micro - 0.2571\n",
      "precision_micro - 0.2571\n",
      "recall_micro - 0.2571\n",
      "\n",
      "MAX PEAK CLASSIFIER SCORES\n",
      "accuracy - 0.8629\n",
      "f1_micro - 0.8629\n",
      "precision_micro - 0.8629\n",
      "recall_micro - 0.8629\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the scores are the same for accuracy, f1, precision and recall which seems odd to me. Why is this the case? Am I doing something wrong? Defo want to check this with Waylon. \n",
    "\n",
    "Let's also run a KFold and see if it makes a difference. I know I have made these a bit unfair since I have used all the data to make my models... eek should I go back and change that?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print('RANDOM CLASSIFIER KFOLD SCORES')\n",
    "print_kfold_scores(random_clf, X, y, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RANDOM CLASSIFIER KFOLD SCORES\n",
      "FOLD 1\n",
      "accuracy - 0.1714\n",
      "f1_micro - 0.1714\n",
      "precision_micro - 0.1714\n",
      "recall_micro - 0.1714\n",
      "\n",
      "FOLD 2\n",
      "accuracy - 0.3143\n",
      "f1_micro - 0.3143\n",
      "precision_micro - 0.3143\n",
      "recall_micro - 0.3143\n",
      "\n",
      "FOLD 3\n",
      "accuracy - 0.1714\n",
      "f1_micro - 0.1714\n",
      "precision_micro - 0.1714\n",
      "recall_micro - 0.1714\n",
      "\n",
      "FOLD 4\n",
      "accuracy - 0.3429\n",
      "f1_micro - 0.3429\n",
      "precision_micro - 0.3429\n",
      "recall_micro - 0.3429\n",
      "\n",
      "FOLD 5\n",
      "accuracy - 0.3429\n",
      "f1_micro - 0.3429\n",
      "precision_micro - 0.3429\n",
      "recall_micro - 0.3429\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print('MAX PEAK CLASSIFIER KFOLD SCORES')\n",
    "print_kfold_scores(max_peak_clf, X, y, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MAX PEAK CLASSIFIER KFOLD SCORES\n",
      "FOLD 1\n",
      "accuracy - 0.6286\n",
      "f1_micro - 0.6286\n",
      "precision_micro - 0.6286\n",
      "recall_micro - 0.6286\n",
      "\n",
      "FOLD 2\n",
      "accuracy - 0.8286\n",
      "f1_micro - 0.8286\n",
      "precision_micro - 0.8286\n",
      "recall_micro - 0.8286\n",
      "\n",
      "FOLD 3\n",
      "accuracy - 0.9143\n",
      "f1_micro - 0.9143\n",
      "precision_micro - 0.9143\n",
      "recall_micro - 0.9143\n",
      "\n",
      "FOLD 4\n",
      "accuracy - 0.9714\n",
      "f1_micro - 0.9714\n",
      "precision_micro - 0.9714\n",
      "recall_micro - 0.9714\n",
      "\n",
      "FOLD 5\n",
      "accuracy - 0.9714\n",
      "f1_micro - 0.9714\n",
      "precision_micro - 0.9714\n",
      "recall_micro - 0.9714\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random classifier ranges from 0.1429 - 0.2286\n",
    "Max peak classifier ranges from 0.6286 - 0.9714"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def calc_average_kfold_accuracy(model, n_splits=5):\n",
    "    kfold = KFold(n_splits=5)\n",
    "    fold = 1\n",
    "    accuracies = []\n",
    "    for _, test_idx in kfold.split(X):\n",
    "        X_test, y_test = X.iloc[test_idx, :], y[test_idx]\n",
    "        acc = accuracy_score(y_test, model.predict(X_test))\n",
    "        accuracies.append(acc)\n",
    "    return np.mean(accuracies)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "calc_average_kfold_accuracy(random_clf)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.26285714285714284"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "calc_average_kfold_accuracy(max_peak_clf)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8628571428571428"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The average accuracies for each model are the same as if the model makes predictions on the whole dataset. This makes sense.\n",
    "\n",
    "I did not split the dataset into train/val/test sets before creating these baseline models. For the `RandomClassifier` this is not an issue since the prediction is not based on the sample. However, for the `MaxPeakClassifier` it will have caused some data leakage - I used the whole population to create these rules and, if I just used a training set, I would not necessarily have come up with the same rules. However, I decided to do this for brevity and to get a better understanding of the dataset as a whole. So, the 86.29% accuracy result is probably an overexaggeration of the model's power.But this is a decent baseline from which to compare everything else to. 86% is very strong for a rule based approach and the best an ML model can hope to achieve is a 16.3% increase in performance. \n",
    "\n",
    "Yes there could be some data leakage in here but really only in the value for seawater we would choose and it's clear that the vast majority 90%+ of our seawater samples stay below 2.5. Moreover, the peak sizes were chosen to be wide enough to have room for all samples. So probably a bit of data leakage but for brevity we will work with it and can say that 86% is the minimum that our ML models need to achieve. \n",
    "\n",
    "Also the train and test sets should come from a similar population. There is no randomness in these models but we can see from KFold that different datasets give different results."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "b60a01ecf532b2f759cce798974887fb1836b7786c6743edae9ea9f1cf50a2f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}