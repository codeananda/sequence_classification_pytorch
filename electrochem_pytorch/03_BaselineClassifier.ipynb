{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline Classifier\n",
    "\n",
    "In this notebook, I will create two baseline classifiers. One random and one intelligent that will make predictions based off the peak of the sample in the ranges I found in 02_DataExploration.ipynb. \n",
    "\n",
    "The peaks for each analyte are at:\n",
    "- Copper - [660, 720]\n",
    "- Cadmium - [530, 580]\n",
    "- Lead - [580, 620]\n",
    "- Seawater - N/A (it's a flat baseline)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Classifier\n",
    "\n",
    "Let's see how the model will perform if it guesses the classes 0-3 at random."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from functools import partial\n",
    "\n",
    "from scripts.data_processing import get_class_label_to_int_mapping\n",
    "\n",
    "CLASS_LABEL_TO_INT_MAPPING = get_class_label_to_int_mapping()\n",
    "\n",
    "sns.set()\n",
    "\n",
    "DATA_DIR = Path('data/')\n",
    "\n",
    "# Read in dataframes - good to keep them separate to make plotting easier\n",
    "cadmium = pd.read_csv(DATA_DIR / 'cadmium.csv', index_col=0)\n",
    "copper = pd.read_csv(DATA_DIR / 'copper.csv', index_col=0)\n",
    "lead = pd.read_csv(DATA_DIR / 'lead.csv', index_col=0)\n",
    "seawater = pd.read_csv(DATA_DIR / 'seawater.csv', index_col=0)\n",
    "\n",
    "analytes = pd.read_csv(DATA_DIR / 'all_data.csv', index_col=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class RandomClassifier:\n",
    "    \"\"\"Returns random predictions of ints in the range 0-3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed=0):\n",
    "        np.random.seed(seed)\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        # Return array len(X) containing ints 0-3 randomly chosen\n",
    "        return np.random.randint(0, 4, len(X))\n",
    "\n",
    "\n",
    "class MaxPeakClassifier:\n",
    "    \"\"\"Returns predictions based off of the maximum peak value of the sample. \n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Blank method to enable use with sklearn API.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"Predict classes 0-3 for X based off of the max peak value.\n",
    "\n",
    "        From EDA, we know that the peaks for each analyte occur in the\n",
    "        following index ranges:\n",
    "        - Cd - (530, 580)\n",
    "        - Cu - (660, 720)\n",
    "        - Pb - (580, 620)\n",
    "\n",
    "        This method checks the ranges and predicts the analyte with the \n",
    "        highest value. However, if the average value of each peak is less \n",
    "        than 2.5, it predicts seawater (this value was found through EDA \n",
    "        too).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray or pd.Series or pd.DataFrame\n",
    "            The object containing voltage series for each analyte. \n",
    "            Should be of shape (n_samples, 1002) since each series has len\n",
    "            1002. Minimum length needed is 720 as that is the final peak\n",
    "            index position that is checked.\n",
    "\n",
    "            Note: you can also input a single series with shape (1002,).\n",
    "        y : None, optional\n",
    "            Placeholder for y to maintain compatibility with sklearn, by default None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            1D array containing predictions for each sample as ints. \n",
    "            The mapping is:\n",
    "                Cd - 0\n",
    "                Cu - 1\n",
    "                Pb - 2\n",
    "                Sw - 3\n",
    "        \"\"\"\n",
    "        # Turn Series and DataFrames into NumPy arrays\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.values\n",
    "        if X.ndim == 1:\n",
    "            # Reshape single sample 1D inputs to 2D\n",
    "            X = X.reshape(1, -1)\n",
    "        # Define peak value ranges\n",
    "        cu_peak_start = 660\n",
    "        cu_peak_end = 720\n",
    "\n",
    "        cd_peak_start = 530\n",
    "        cd_peak_end = 580\n",
    "\n",
    "        pb_peak_start = 580\n",
    "        pb_peak_end = 620\n",
    "\n",
    "        preds = []\n",
    "        # Iterate over the rows (i.e. samples) in X\n",
    "        for sample in X:\n",
    "            # Get peak values from sample\n",
    "            cu_peak_value = sample[cu_peak_start:cu_peak_end].max()\n",
    "            cd_peak_value = sample[cd_peak_start:cd_peak_end].max()\n",
    "            pb_peak_value = sample[pb_peak_start:pb_peak_end].max()\n",
    "\n",
    "            peak_values = {\n",
    "                'Cd': cd_peak_value,\n",
    "                'Cu': cu_peak_value,\n",
    "                'Pb': pb_peak_value\n",
    "                }\n",
    "\n",
    "            avergae_value = np.mean(list(peak_values.values()))\n",
    "\n",
    "            # On average, seawater max values are less than 2.5, found by inspection\n",
    "            if avergae_value <= 2.5:\n",
    "                pred = CLASS_LABEL_TO_INT_MAPPING['Sw']\n",
    "                preds.append(pred)\n",
    "            else:\n",
    "                analyte_with_biggest_peak = max(peak_values, key=peak_values.get)\n",
    "                pred = CLASS_LABEL_TO_INT_MAPPING[analyte_with_biggest_peak]\n",
    "                preds.append(pred)\n",
    "        return np.array(preds)\n",
    "\n",
    "\n",
    "def print_scores(model, X, y, average='micro'):\n",
    "    \"\"\"Print accuracy, precision, recall and f1 scores for a given\n",
    "    fit model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : A fitted model with a predict() method\n",
    "        A model that has already been trained on data and is ready\n",
    "        to make predictions\n",
    "    X : np.ndarray, pd.Series, pd.DataFrame\n",
    "        Input array containing samples with shape (n_samples, n_features)\n",
    "    y : np.ndarray or pd.Series\n",
    "        True predictions\n",
    "    average : str, optional\n",
    "        Determines the averaging performed on the data. Required for \n",
    "        multiclass classification. Options are: 'micro' (default),\n",
    "        ‘micro’, ‘macro’, ‘samples’, ‘weighted’.\n",
    "    \"\"\"\n",
    "    preds = model.predict(X)\n",
    "    \n",
    "    scores = {\n",
    "        'accuracy': accuracy_score, \n",
    "        'f1': partial(f1_score, average='micro'), \n",
    "        'precision': partial(precision_score, average='micro'),\n",
    "        'recall': partial(recall_score, average='micro')\n",
    "    }\n",
    "\n",
    "    for name, score_func in scores.items():\n",
    "        if name != 'accuracy':\n",
    "            print(f'{name}_{average} - {score_func(y, preds):.4f}')\n",
    "        else:\n",
    "            print(f'{name} - {score_func(y, preds):.4f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "X = analytes.drop('label', axis=1)\n",
    "y = analytes.loc[:, 'label']\n",
    "\n",
    "random_clf = RandomClassifier()\n",
    "print('RANDOM CLASSIFIER SCORES')\n",
    "print_scores(random_clf, X, y)\n",
    "print()\n",
    "\n",
    "max_peak_clf = MaxPeakClassifier()\n",
    "print('MAX PEAK CLASSIFIER SCORES')\n",
    "print_scores(max_peak_clf, X, y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RANDOM CLASSIFIER SCORES\n",
      "accuracy - 0.2571\n",
      "f1_micro - 0.2571\n",
      "precision_micro - 0.2571\n",
      "recall_micro - 0.2571\n",
      "\n",
      "MAX PEAK CLASSIFIER SCORES\n",
      "accuracy - 0.8629\n",
      "f1_micro - 0.8629\n",
      "precision_micro - 0.8629\n",
      "recall_micro - 0.8629\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the scores are the same for accuracy, f1, precision and recall which seems odd to me. Why is this the case? Am I doing something wrong? Defo want to check this with Waylon. \n",
    "\n",
    "Let's also run a KFold and see if it makes a difference. I know I have made these a bit unfair since I have used all the data to make my models... eek should I go back and change that?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def print_kfold_scores(model, X, y, n_splits=5, **kwargs):\n",
    "    \"\"\"Runs KFold validation n_splits times and prints the accuracy,\n",
    "    f1, precision and recall scores for each fold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Model with fit and predict methods\n",
    "        The model to use for training and predicting\n",
    "    X : np.ndarray, pd.Series, pd.DataFrame\n",
    "        Input array containing samples with shape (n_samples, 1002)\n",
    "    y : np.ndarray or pd.Series\n",
    "        Output array\n",
    "    n_splits : int\n",
    "        The number of folds to create, defaults to 5\n",
    "    \"\"\"\n",
    "    kfold = KFold(n_splits=n_splits)\n",
    "    fold = 1\n",
    "    for train_idx, test_idx in kfold.split(X):\n",
    "        print(f'FOLD {fold}')\n",
    "        X_train, y_train = X.iloc[train_idx, :], y[train_idx]\n",
    "        X_test, y_test = X.iloc[test_idx, :], y[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        print_scores(model, X_test, y_test, **kwargs)\n",
    "        print()\n",
    "        fold += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print('RANDOM CLASSIFIER KFOLD SCORES')\n",
    "print_kfold_scores(random_clf, X, y, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RANDOM CLASSIFIER KFOLD SCORES\n",
      "FOLD 1\n",
      "accuracy - 0.1714\n",
      "f1_micro - 0.1714\n",
      "precision_micro - 0.1714\n",
      "recall_micro - 0.1714\n",
      "\n",
      "FOLD 2\n",
      "accuracy - 0.3143\n",
      "f1_micro - 0.3143\n",
      "precision_micro - 0.3143\n",
      "recall_micro - 0.3143\n",
      "\n",
      "FOLD 3\n",
      "accuracy - 0.1714\n",
      "f1_micro - 0.1714\n",
      "precision_micro - 0.1714\n",
      "recall_micro - 0.1714\n",
      "\n",
      "FOLD 4\n",
      "accuracy - 0.3429\n",
      "f1_micro - 0.3429\n",
      "precision_micro - 0.3429\n",
      "recall_micro - 0.3429\n",
      "\n",
      "FOLD 5\n",
      "accuracy - 0.3429\n",
      "f1_micro - 0.3429\n",
      "precision_micro - 0.3429\n",
      "recall_micro - 0.3429\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print('MAX PEAK CLASSIFIER KFOLD SCORES')\n",
    "print_kfold_scores(max_peak_clf, X, y, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MAX PEAK CLASSIFIER KFOLD SCORES\n",
      "FOLD 1\n",
      "accuracy - 0.6286\n",
      "f1_micro - 0.6286\n",
      "precision_micro - 0.6286\n",
      "recall_micro - 0.6286\n",
      "\n",
      "FOLD 2\n",
      "accuracy - 0.8286\n",
      "f1_micro - 0.8286\n",
      "precision_micro - 0.8286\n",
      "recall_micro - 0.8286\n",
      "\n",
      "FOLD 3\n",
      "accuracy - 0.9143\n",
      "f1_micro - 0.9143\n",
      "precision_micro - 0.9143\n",
      "recall_micro - 0.9143\n",
      "\n",
      "FOLD 4\n",
      "accuracy - 0.9714\n",
      "f1_micro - 0.9714\n",
      "precision_micro - 0.9714\n",
      "recall_micro - 0.9714\n",
      "\n",
      "FOLD 5\n",
      "accuracy - 0.9714\n",
      "f1_micro - 0.9714\n",
      "precision_micro - 0.9714\n",
      "recall_micro - 0.9714\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random classifier ranges from 0.1429 - 0.2286\n",
    "Max peak classifier ranges from 0.6286 - 0.9714"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def calc_average_kfold_accuracy(model, n_splits=5):\n",
    "    kfold = KFold(n_splits=5)\n",
    "    fold = 1\n",
    "    accuracies = []\n",
    "    for _, test_idx in kfold.split(X):\n",
    "        X_test, y_test = X.iloc[test_idx, :], y[test_idx]\n",
    "        acc = accuracy_score(y_test, model.predict(X_test))\n",
    "        accuracies.append(acc)\n",
    "    return np.mean(accuracies)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "calc_average_kfold_accuracy(random_clf)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.26285714285714284"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "calc_average_kfold_accuracy(max_peak_clf)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8628571428571428"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The average accuracies for each model are the same as if the model makes predictions on the whole dataset. This makes sense.\n",
    "\n",
    "I did not split the dataset into train/val/test sets before creating these baseline models. For the `RandomClassifier` this is not an issue since the prediction is not based on the sample. However, for the `MaxPeakClassifier` it will have caused some data leakage - I used the whole population to create these rules and, if I just used a training set, I would not necessarily have come up with the same rules. However, I decided to do this for brevity and to get a better understanding of the dataset as a whole. So, the 86.29% accuracy result is probably an overexaggeration of the model's power.But this is a decent baseline from which to compare everything else to. 86% is very strong for a rule based approach and the best an ML model can hope to achieve is a 16.3% increase in performance. \n",
    "\n",
    "Yes there could be some data leakage in here but really only in the value for seawater we would choose and it's clear that the vast majority 90%+ of our seawater samples stay below 2.5. Moreover, the peak sizes were chosen to be wide enough to have room for all samples. So probably a bit of data leakage but for brevity we will work with it and can say that 86% is the minimum that our ML models need to achieve. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "b60a01ecf532b2f759cce798974887fb1836b7786c6743edae9ea9f1cf50a2f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}